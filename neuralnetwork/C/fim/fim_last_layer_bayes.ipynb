{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to compute the FIM of an NN potential for C with respect to the  weights and biases corresponding to the mapping from the last hidden layer to the output layer.\n",
    "The FIM is calculated using the posterior to regularize the singularity of the FIM calculated using the likelihood only.\n",
    "Since we add a prior, then this method is more Bayesian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T20:29:31.079986Z",
     "start_time": "2024-06-10T20:29:19.993684Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import numdifftools as nd\n",
    "import torch\n",
    "\n",
    "from kliff import nn\n",
    "from kliff.calculators import CalculatorTorch\n",
    "from kliff.dataset import Dataset\n",
    "from kliff.dataset.weight import Weight\n",
    "from kliff.descriptors import SymmetryFunction\n",
    "from kliff.loss import Loss\n",
    "from kliff.models import NeuralNetwork\n",
    "\n",
    "# Random seed\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "# torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use(\"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T20:29:31.333688Z",
     "start_time": "2024-06-10T20:29:31.142715Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read setting file\n",
    "WORK_DIR = Path().absolute()\n",
    "ROOT_DIR = WORK_DIR.parent\n",
    "DATA_DIR = ROOT_DIR / \"data\"\n",
    "with open(ROOT_DIR / \"settings.json\", \"r\") as f:\n",
    "    settings = json.load(f)\n",
    "partition = settings[\"partition\"]\n",
    "suffix = \"_\".join([str(n) for n in settings[\"Nnodes\"]])\n",
    "PART_DIR = DATA_DIR / f\"{partition}_partition_data\"\n",
    "FP_DIR = PART_DIR / \"fingerprints\"\n",
    "RES_DIR = WORK_DIR / \"results\" / f\"{partition}_partition_{suffix}\"\n",
    "if not RES_DIR.exists():\n",
    "    RES_DIR.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T20:29:31.363326Z",
     "start_time": "2024-06-10T20:29:31.349745Z"
    }
   },
   "outputs": [],
   "source": [
    "# Architecture\n",
    "Nlayers = settings[\"Nlayers\"]  # Number of layers, excluding input layer, including outpt layer\n",
    "Nnodes = settings[\"Nnodes\"]  # Number of nodes per hidden layer\n",
    "dropout_ratio = 0.0  # Don't use dropout\n",
    "\n",
    "# Optimizer settings\n",
    "learning_rate = 1e-3\n",
    "batch_size = 100\n",
    "nepochs_total = 40_000  # How many epochs to run in total\n",
    "nepochs_initial = 2000  # Run this many epochs first\n",
    "nepochs_save_period = 10  # Then run and save every this many epochs\n",
    "epoch_change_lr = 5000  # This is the epoch when we change the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T20:29:31.419186Z",
     "start_time": "2024-06-10T20:29:31.368687Z"
    }
   },
   "outputs": [],
   "source": [
    "# Descriptor\n",
    "descriptor = SymmetryFunction(\n",
    "    cut_name=\"cos\", cut_dists={\"C-C\": 5.0}, hyperparams=\"set51\", normalize=True\n",
    ")\n",
    "model = NeuralNetwork(descriptor)\n",
    "\n",
    "# Layers\n",
    "hidden_layer_mappings = []\n",
    "for ii in range(Nlayers - 2):\n",
    "    hidden_layer_mappings.append(nn.Dropout(dropout_ratio))\n",
    "    hidden_layer_mappings.append(nn.Linear(Nnodes[ii], Nnodes[ii + 1]))\n",
    "    hidden_layer_mappings.append(nn.Tanh())\n",
    "\n",
    "model.add_layers(\n",
    "    # input layer\n",
    "    nn.Linear(descriptor.get_size(), Nnodes[0]),  # Mapping from input layer to the first\n",
    "    nn.Tanh(),  # hidden layer\n",
    "    # hidden layer(s)\n",
    "    *hidden_layer_mappings,  # Mappings between hidden layers in the middle\n",
    "    # hidden layer(s)\n",
    "    nn.Dropout(dropout_ratio),  # Mapping from the last hidden layer to the output layer\n",
    "    nn.Linear(Nnodes[-1], 1),\n",
    "    # output layer\n",
    ")\n",
    "\n",
    "# Load best model\n",
    "orig_model_path = (\n",
    "    ROOT_DIR\n",
    "    / \"training_dropout\"\n",
    "    / \"results\"\n",
    "    / \"training\"\n",
    "    / f\"{partition}_partition_{suffix}\"\n",
    "    / \"model_best_train.pkl\"\n",
    ")\n",
    "model.load(orig_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training set and calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T20:30:30.970642Z",
     "start_time": "2024-06-10T20:29:31.423680Z"
    }
   },
   "outputs": [],
   "source": [
    "# training set\n",
    "dataset_path = PART_DIR / \"carbon_training_set\"\n",
    "weight = Weight(energy_weight=1.0, forces_weight=np.sqrt(0.1))\n",
    "tset = Dataset(dataset_path, weight)\n",
    "configs = tset.get_configs()\n",
    "nconfigs = len(configs)\n",
    "\n",
    "# calculator\n",
    "gpu = False\n",
    "calc = CalculatorTorch(model, gpu=gpu)\n",
    "_ = calc.create(\n",
    "    configs,\n",
    "    nprocs=20,\n",
    "    reuse=True,\n",
    "    fingerprints_filename=FP_DIR / f\"fingerprints_train.pkl\",\n",
    "    fingerprints_mean_stdev_filename=FP_DIR / f\"fingerprints_train_mean_and_stdev.pkl\",\n",
    ")\n",
    "bestfit_params = calc.get_opt_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T20:30:30.986107Z",
     "start_time": "2024-06-10T20:30:30.976782Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loss function\n",
    "residual_data = {\"normalize_by_natoms\": True}\n",
    "loss = Loss(calc, residual_data=residual_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T20:30:30.998977Z",
     "start_time": "2024-06-10T20:30:30.989772Z"
    }
   },
   "outputs": [],
   "source": [
    "# loader = calc.get_compute_arguments(batch_size)\n",
    "loader = calc.get_compute_arguments(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get reference data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T20:30:31.990798Z",
     "start_time": "2024-06-10T20:30:31.200213Z"
    }
   },
   "outputs": [],
   "source": [
    "reference_energy = []\n",
    "energy_natoms = []\n",
    "for batch in loader:\n",
    "    batch_energy = np.array([float(sample[\"energy\"]) for sample in batch])\n",
    "    batch_natoms = np.array([sample[\"forces\"].shape[0] for sample in batch])\n",
    "    reference_energy = np.append(reference_energy, batch_energy)\n",
    "    energy_natoms = np.append(energy_natoms, batch_natoms)\n",
    "print(len(reference_energy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T20:31:05.243631Z",
     "start_time": "2024-06-10T20:30:32.029073Z"
    }
   },
   "outputs": [],
   "source": [
    "reference_forces = []\n",
    "forces_natoms = []\n",
    "for batch in loader:\n",
    "    batch_forces = [sample[\"forces\"].numpy().flatten() for sample in batch]\n",
    "    batch_natoms = [\n",
    "        np.repeat(sample[\"forces\"].shape[0], np.prod(sample[\"forces\"].shape))\n",
    "        for sample in batch\n",
    "    ]\n",
    "    reference_forces = np.append(reference_forces, np.concatenate(batch_forces))\n",
    "    forces_natoms = np.append(forces_natoms, np.concatenate(batch_natoms))\n",
    "print(reference_forces.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T20:31:05.260367Z",
     "start_time": "2024-06-10T20:31:05.250743Z"
    }
   },
   "outputs": [],
   "source": [
    "# Total number of predictions\n",
    "npreds = len(reference_energy) + len(reference_forces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the Jacobian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we are only considering the parameters of the last (output) layer, then the model is actually linear with respect to these parameters.\n",
    "The energy of atom $i$ is calculated by\n",
    "\\begin{equation}\n",
    "    E_i = W \\phi_i + b,\n",
    "\\end{equation}\n",
    "where $W$ is a vector of the weights of the last layer and $b$ is the bias, and $\\phi$ is the input to the last layer.\n",
    "From this, we can see that\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial E_i}{\\partial w_j} = (\\phi_i)_j\n",
    "    \\quad \\text{and} \\quad\n",
    "    \\frac{\\partial E_i}{\\partial b} = 1.\n",
    "\\end{equation}\n",
    "However, note that to compute the configuration energy, we need to sum the energy over all atoms in the configuration.\n",
    "\n",
    "From this model, we compute the force element of atom $i$ by\n",
    "\\begin{equation}\n",
    "    F^i_x = \\frac{\\partial E_i}{\\partial r^i_x}\n",
    "    = \\frac{\\partial E_i}{\\partial \\zeta} \\frac{\\partial \\zeta}{\\partial r^i_x},\n",
    "\\end{equation}\n",
    "where $\\zeta$ is the atomic descriptor.\n",
    "Note that using the property of derivative,\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial F^i_x}{\\partial w_j} = \\frac{\\partial \\zeta}{\\partial r^i_x} \\frac{\\partial}{\\partial \\zeta} \\left( \\frac{\\partial E_i}{\\partial w_j} \\right)\n",
    "    = \\frac{\\partial \\zeta}{\\partial r^i_x} \\frac{\\partial (\\phi_i)_j}{\\partial \\zeta},\n",
    "\\end{equation}\n",
    "which is the input to the last layer when we use the model to compute the force.\n",
    "Additionally, notice that\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial F^i_x}{\\partial b} = 0.\n",
    "\\end{equation}\n",
    "\n",
    "The FIM for this model is calculated by\n",
    "\\begin{equation}\n",
    "    \\mathcal{I} = J^T J,\n",
    "\\end{equation}\n",
    "where $J$ is the Jacobian matrix of the _residual_,\n",
    "\\begin{equation}\n",
    "    r(\\theta) = \\frac{y - f(\\theta)}{\\sigma}.\n",
    "\\end{equation}\n",
    "In this equation $y$ represents the reference data, $f(\\theta)$ represents the model predictions, and $\\sigma$ is the inverse weight of the residual.\n",
    "Notice that the element of the Jacobian matrix is\n",
    "\\begin{equation}\n",
    "    J_{ij} = \\frac{1}{\\sigma} \\frac{\\partial f_i}{\\partial \\theta_j},\n",
    "\\end{equation}\n",
    "i.e., we need to include th weights in the derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T20:31:05.275928Z",
     "start_time": "2024-06-10T20:31:05.265696Z"
    }
   },
   "outputs": [],
   "source": [
    "device = model.device\n",
    "layers_no_output = model.layers[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T20:31:05.507578Z",
     "start_time": "2024-06-10T20:31:05.281278Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute the derivative of the energy\n",
    "de_file = Path(RES_DIR / \"jacobian_energy.npy\")\n",
    "\n",
    "if de_file.exists():\n",
    "    de_all = np.load(de_file)\n",
    "else:\n",
    "    energy_weight = 1.0\n",
    "    # Note that there is still an extra weight factor related to the number of atoms\n",
    "    de_all = np.empty((0, 129))  # There are 128 weights and 1 bias\n",
    "\n",
    "    for batch in tqdm(loader):\n",
    "        # Get the fingerprints - these lines are from calc.compute\n",
    "        zeta_config = [sample[\"zeta\"] for sample in batch]  # Zeta for each config\n",
    "        zeta_stacked = torch.cat(zeta_config, dim=0).to(device)\n",
    "        zeta_stacked.requires_grad_(True)\n",
    "        natoms_config = [len(zeta) for zeta in zeta_config]  # Number of atoms per config\n",
    "\n",
    "        # Try to retrieve input values of the last hidden layer\n",
    "        x_atom = torch.clone(zeta_stacked)\n",
    "        for layer in layers_no_output:\n",
    "            x_atom = layer(x_atom)\n",
    "        x_config = [e.sum(0) for e in torch.split(x_atom, natoms_config)]\n",
    "        # Derivative of the energy with respect to the weights is just this input values\n",
    "        de = np.array([elem.detach().numpy() for elem in x_config])\n",
    "        # Normalize by number of atoms\n",
    "        de/= np.array(natoms_config).reshape((-1, 1))\n",
    "        # Add the derivative of energy with respect to the bias\n",
    "        # Before normalizing by the number of atoms, this value should be the same as\n",
    "        # the number of atoms. But, we will divide it with the number of atoms anyway.\n",
    "        de = np.column_stack((de, np.ones(len(de))))\n",
    "        # Finally, multiply the derivative by the weight\n",
    "        de *= energy_weight\n",
    "\n",
    "        # Append to the container matrix\n",
    "        de_all = np.row_stack((de_all, de))\n",
    "\n",
    "    # Export\n",
    "    np.save(RES_DIR / \"jacobian_energy.npy\", de_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T20:31:27.049471Z",
     "start_time": "2024-06-10T20:31:05.565754Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute the derivative of the energy\n",
    "df_file = Path(RES_DIR / \"jacobian_forces.npy\")\n",
    "\n",
    "# Unlike the calculation for the derivative of the energy, this calculation took me\n",
    "# about 5 minutes. Just loading the data even took about 5s.\n",
    "if df_file.exists():\n",
    "    df_all = np.load(df_file)\n",
    "else:\n",
    "    forces_weight = np.sqrt(0.1)\n",
    "    # Note that there is still an extra weight factor related to the number of atoms\n",
    "    df_all = np.empty((0, 129))  # There are 128 weights and 1 bias\n",
    "\n",
    "    for batch in tqdm(loader):\n",
    "        # Get the fingerprints - these lines are from calc.compute\n",
    "        zeta_config = [sample[\"zeta\"] for sample in batch]  # Zeta for each config\n",
    "        zeta_stacked = torch.cat(zeta_config, dim=0).to(device)\n",
    "        zeta_stacked.requires_grad_(True)\n",
    "        natoms_config = [\n",
    "            len(zeta) for zeta in zeta_config\n",
    "        ]  # Number of atoms per config\n",
    "\n",
    "        # Try to retrieve values from the last hidden layer\n",
    "        x_atom = torch.clone(zeta_stacked)\n",
    "        for layer in layers_no_output:\n",
    "            x_atom = layer(x_atom)\n",
    "\n",
    "        x_config = [e.sum(0) for e in torch.split(x_atom, natoms_config)]\n",
    "        # Derivative of the energy with respect to the weights\n",
    "        de = np.array([elem.detach().numpy() for elem in x_config])\n",
    "        # Add the derivative of energy with respect to the bias\n",
    "        de = np.column_stack((de, np.ones(len(de))))\n",
    "\n",
    "        # Derivative of the forces with respect to all weights of the last hidden layer.\n",
    "        # This is the same as the input values of the last hidden layer for forces\n",
    "        # predictions.\n",
    "        df = []\n",
    "        for x_atom_node in x_atom.T:\n",
    "            # de/dzeta for the each node of the last hidden layer\n",
    "            x_atom_node = x_atom_node.reshape((-1, 1))\n",
    "            dedzeta = torch.autograd.grad(\n",
    "                x_atom_node.sum(), zeta_stacked, create_graph=True\n",
    "            )[0]\n",
    "            dedzeta_config = torch.split(dedzeta, natoms_config)\n",
    "\n",
    "            # df/dzeta for the each node of the last hidden layer\n",
    "            forces_config = []\n",
    "            for i, sample in enumerate(batch):\n",
    "                dedzeta = dedzeta_config[i]\n",
    "\n",
    "                dzetadr_forces = sample[\"dzetadr_forces\"].to(device)\n",
    "                f = calc._compute_forces(dedzeta, dzetadr_forces)\n",
    "                # Normalize by number of atoms\n",
    "                f /= sample[\"configuration\"].get_num_atoms()\n",
    "                forces_config.append(f)\n",
    "            forces_config = torch.cat(forces_config)\n",
    "            df.append(forces_config)\n",
    "\n",
    "        # Stack the forces\n",
    "        df = torch.stack(df).T\n",
    "        zeta_stacked.requires_grad_(False)\n",
    "\n",
    "        # Convert to numpy\n",
    "        df = df.detach().numpy()\n",
    "        # Add the derivative with respect to the bias, which is just zero\n",
    "        df = np.column_stack((df, np.zeros(len(df))))\n",
    "        # Finally, multiply the derivative by the weight\n",
    "        df *= forces_weight\n",
    "\n",
    "        # Append to the container matrix\n",
    "        df_all = np.row_stack((df_all, df))\n",
    "\n",
    "    # Export\n",
    "    np.save(df_file, df_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the FIM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivation of the FIM with using the posterior\n",
    "* Posterior\n",
    "    \\begin{align*}\n",
    "        P(\\theta | y) &= \\frac{L(\\theta | y) \\pi(\\theta)}{P(y)} \\\\\n",
    "        L(\\theta | y) &= \\frac{1}{N_L} \\exp{\\left[ -\\frac{1}{2} \\sum_m \\left( \\frac{y_m - f_m(\\theta)}{\\sigma_m} \\right)^2 \\right]} \\\\\n",
    "        \\pi(\\theta) &= \\frac{1}{N_\\pi} \\exp{\\left[ -\\frac{1}{2} \\sum_n \\left( \\frac{\\theta_n - \\theta_n^*}{s_n} \\right)^2 \\right]}\n",
    "    \\end{align*}\n",
    "\n",
    "* Log-posterior\n",
    "    \\begin{equation*}\n",
    "        \\log [P(\\theta | y)] = \\log [L(\\theta | y)] + \\log [\\pi(\\theta)] - \\log [P(y)]\n",
    "    \\end{equation*}\n",
    "    \n",
    "* Hessian of log-posterior\n",
    "    \\begin{equation*}\n",
    "        \\frac{\\partial^2 \\log [P(\\theta | y)]}{\\partial \\theta_\\mu \\partial \\theta_\\nu} = \\frac{\\partial^2 \\log [L(\\theta | y)]}{\\partial \\theta_\\mu \\partial \\theta_\\nu} + \\frac{\\partial^2 \\log [\\pi(\\theta)]}{\\partial \\theta_\\mu \\partial \\theta_\\nu}\n",
    "    \\end{equation*}\n",
    "    $P(y)$ is independent of $\\theta$.\n",
    "    \n",
    "    * Hessian of log-likelihood\n",
    "        \\begin{align*}\n",
    "            \\log [L(\\theta | y)] &= -\\log(N_L) -\\frac{1}{2} \\sum_m \\left( \\frac{y_m - f_m(\\theta)}{\\sigma_m} \\right)^2 \\\\\n",
    "            \\frac{\\partial \\log [L(\\theta | y)]}{\\partial \\theta_\\mu} &= \\sum_m \\left( \\frac{y_m - f_m(\\theta)}{\\sigma_m^2} \\right) \\left( \\frac{\\partial f_m(\\theta)}{\\partial \\theta_\\mu} \\right) \\\\\n",
    "            \\frac{\\partial^2 \\log [L(\\theta | y)]}{\\partial \\theta_\\mu \\partial \\theta_\\nu} &= \\sum_m \\left[ -\\frac{1}{\\sigma_m^2} \\frac{\\partial f_m(\\theta)}{\\partial \\theta_\\mu} \\frac{\\partial f_m(\\theta)}{\\partial \\theta_\\nu} + \\left( \\frac{y_m - f_m(\\theta)}{\\sigma_m^2} \\right) \\frac{\\partial^2 f_m(\\theta)}{\\partial \\theta_\\mu \\partial \\theta_\\nu} \\right]\n",
    "        \\end{align*}\n",
    "\n",
    "    * Hessian of log-prior\n",
    "        \\begin{align*}\n",
    "            \\log [\\pi(\\theta)] &= \\log(N_\\pi) -\\frac{1}{2} \\sum_n \\left( \\frac{\\theta_n - \\theta_n^*}{s_n} \\right)^2 \\\\\n",
    "            \\frac{\\partial \\log [\\pi(\\theta)]}{\\partial \\theta_\\mu} &= -\\left( \\frac{\\theta_\\mu - \\theta_\\mu^*}{s_\\mu^2} \\right) \\\\\n",
    "            \\frac{\\partial^2 \\log [\\pi(\\theta)]}{\\partial \\theta_\\mu \\partial \\theta_\\nu} &= - \\frac{\\delta_{\\mu \\nu}}{s_\\mu^2}\n",
    "        \\end{align*}\n",
    "        \n",
    "* Negative expectation value of the Hessian - FIM\n",
    "    \\begin{align*}\n",
    "        -\\left< \\frac{\\partial^2 \\log [P(\\theta | y)]}{\\partial \\theta_\\mu \\partial \\theta_\\nu} \\right> &= -\\left< \\frac{\\partial^2 \\log [L(\\theta | y)]}{\\partial \\theta_\\mu \\partial \\theta_\\nu} \\right> - \\left< \\frac{\\partial^2 \\log [\\pi(\\theta)]}{\\partial \\theta_\\mu \\partial \\theta_\\nu} \\right> \\\\\n",
    "        &= \\sum_m \\left[ \\frac{1}{\\sigma_m^2} \\frac{\\partial f_m(\\theta)}{\\partial \\theta_\\mu} \\frac{\\partial f_m(\\theta)}{\\partial \\theta_\\nu} \\right] + \\frac{\\delta_{\\mu \\nu}}{s_\\mu^2}\n",
    "    \\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T20:31:31.140871Z",
     "start_time": "2024-06-10T20:31:27.061068Z"
    }
   },
   "outputs": [],
   "source": [
    "# Log-likelihood term\n",
    "fim_energy = de_all.T @ de_all\n",
    "fim_forces = df_all.T @ df_all\n",
    "fim_like = fim_energy + fim_forces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T20:31:31.170096Z",
     "start_time": "2024-06-10T20:31:31.151340Z"
    }
   },
   "outputs": [],
   "source": [
    "# Log-prior term - A Xavier normal prior, which is the distribution we use to sample\n",
    "# the initial weights and biases. However, we will center it around the best fit,\n",
    "# instead of the origin.\n",
    "fan_in = model.layers[-1].in_features  # Number of input of the last layer\n",
    "fan_out = model.layers[-1].out_features  # Number of output of the last layer\n",
    "s = np.sqrt(2 / (fan_in + fan_out))  # standard deviation of the Xavier normal prior\n",
    "fim_prior = np.diag(np.ones(len(fim_like)) / (s ** 2))\n",
    "\n",
    "# # Log-prior term - Standard normal distribution\n",
    "# fim_prior = np.diag(np.ones(len(fim_like)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T20:42:14.859282Z",
     "start_time": "2024-06-10T20:31:31.177235Z"
    }
   },
   "outputs": [],
   "source": [
    "# FIM using posterior\n",
    "# Temperature term\n",
    "nparams = 129\n",
    "C0 = 0.5 * loss._get_loss_epoch(loader)  # This loss value doesn't have a factor of 1/2\n",
    "# T = 2 * C0 / nparams  # Natural temperature\n",
    "# T = 2 * C0 / (npreds - nparams)  # Unbiased estimate of variance\n",
    "T = 1.0\n",
    "print(\"Use temperature hyperparameter\", T)\n",
    "# FIM using posterior\n",
    "fim_configs = fim_like / T + fim_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T20:42:25.376162Z",
     "start_time": "2024-06-10T20:42:14.864103Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compare the eigenvalues\n",
    "eigvals_like = scipy.linalg.eigvalsh(fim_like)\n",
    "eigvals_post = scipy.linalg.eigvalsh(fim_configs)\n",
    "# print(eigvals)\n",
    "\n",
    "plt.figure()\n",
    "for lam1, lam2 in zip(eigvals_like, eigvals_post):\n",
    "    plt.plot([-0.4, 0.4], [lam1, lam1], c=\"k\")\n",
    "    plt.plot([0.6, 1.4], [lam2, lam2], c=\"b\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xticks([0, 1], [\"Likelihood\", \"Posterior\"])\n",
    "plt.ylabel(\"Eigenvalue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T20:42:26.313054Z",
     "start_time": "2024-06-10T20:42:25.390801Z"
    }
   },
   "outputs": [],
   "source": [
    "# Covariance\n",
    "cov = scipy.linalg.pinvh(fim_configs)\n",
    "np.save(RES_DIR / \"covariance_parameters.npy\", cov)\n",
    "# scipy.linalg.eigvalsh(cov)\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(np.diag(cov))\n",
    "# plt.yscale(\"log\")\n",
    "# plt.ylim(1e-6, 1)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T20:42:26.568815Z",
     "start_time": "2024-06-10T20:42:26.526231Z"
    }
   },
   "outputs": [],
   "source": [
    "# Parameter samples\n",
    "p0 = bestfit_params[-nparams:]\n",
    "\n",
    "param_samples_file = RES_DIR / \"samples_bayes_parameters.npy\"\n",
    "if param_samples_file.exists():\n",
    "    param_samples = np.load(param_samples_file)\n",
    "else:\n",
    "    param_samples = np.random.multivariate_normal(p0, cov, 100)\n",
    "    np.save(param_samples_file, param_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T20:42:26.597946Z",
     "start_time": "2024-06-10T20:42:26.585217Z"
    }
   },
   "outputs": [],
   "source": [
    "def install_uninstall_model(modelname, mode=\"install\"):\n",
    "    \"\"\"This function will install or remove KIM model.\"\"\"\n",
    "    kim_command = \"kim-api-collections-management\"\n",
    "    if mode == \"install\":\n",
    "        flags = [\"install\", \"user\"]\n",
    "    elif mode == \"uninstall\":\n",
    "        flags = [\"remove\", \"--force\"]\n",
    "        modelname = Path(modelname).name\n",
    "    command = np.concatenate(([kim_command], flags, [modelname]))\n",
    "    subprocess.run(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T20:42:26.623202Z",
     "start_time": "2024-06-10T20:42:26.612728Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write and install model ensembles\n",
    "for ii, params in tqdm(enumerate(param_samples), total=len(param_samples)):\n",
    "    # Write the model\n",
    "    complete_params = bestfit_params.copy()\n",
    "    complete_params[-nparams:] = params\n",
    "    calc.update_model_params(complete_params)  # Update parameters\n",
    "    # Install model\n",
    "    modelpath = RES_DIR / f\"{ii:03d}\" / f\"DUNN_C_fimbayes_{ii:03d}\"\n",
    "    model.write_kim_model(modelpath)  # Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T20:42:26.647426Z",
     "start_time": "2024-06-10T20:42:26.631928Z"
    }
   },
   "outputs": [],
   "source": [
    "def install_uninstall_wrapper(ii):\n",
    "    modelname = RES_DIR / f\"{ii:03d}\" / f\"DUNN_C_fimbayes_{ii:03d}\"\n",
    "    # Uninstall first to make sure we don't use the wrong models later\n",
    "    install_uninstall_model(modelname, \"uninstall\")\n",
    "    # Install this model ensemble\n",
    "    install_uninstall_model(modelname, \"install\")\n",
    "\n",
    "\n",
    "with Pool(25) as p:\n",
    "    p.map(install_uninstall_wrapper, range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
